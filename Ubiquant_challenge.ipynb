{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datatable as dt # data loading\nimport matplotlib.pyplot as plt # data visualizatin\nfrom sys import getsizeof # system interaction\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-26T23:39:14.423438Z","iopub.execute_input":"2022-01-26T23:39:14.424316Z","iopub.status.idle":"2022-01-26T23:39:14.529096Z","shell.execute_reply.started":"2022-01-26T23:39:14.424189Z","shell.execute_reply":"2022-01-26T23:39:14.528195Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_data = dt.fread('../input/ubiquant-market-prediction/train.csv').to_pandas()\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:39:14.531000Z","iopub.execute_input":"2022-01-26T23:39:14.531632Z","iopub.status.idle":"2022-01-26T23:41:12.475070Z","shell.execute_reply.started":"2022-01-26T23:39:14.531585Z","shell.execute_reply":"2022-01-26T23:41:12.474114Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def type_casting(dataset):\n    print('Starting Dataframe size: %2.2f GB'%(getsizeof(dataset)/(1024.0**3)))\n    dataset.time_id = dataset.time_id.astype('uint16')\n    dataset.investment_id = dataset.investment_id.astype('uint16')\n    dataset = dataset.drop(['row_id'],axis = 1)\n    for i in range(300):\n        dataset[f'f_{i}'] = dataset[f'f_{i}'].astype('float32')\n    print('Ending Dataframe size: %2.2f GB'%(getsizeof(dataset)/(1024.0**3)))\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:41:12.476767Z","iopub.execute_input":"2022-01-26T23:41:12.477089Z","iopub.status.idle":"2022-01-26T23:41:12.485174Z","shell.execute_reply.started":"2022-01-26T23:41:12.477042Z","shell.execute_reply":"2022-01-26T23:41:12.484215Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('Starting Dataframe size: %2.2f GB'%(getsizeof(train_data)/(1024.0**3)))\n\ntrain_data = train_data.sample(frac=0.05, replace=True, random_state=1)\n\ntrain_data.time_id = train_data.time_id.astype('uint16')\ntrain_data.investment_id = train_data.investment_id.astype('uint16')\ntrain_data = train_data.drop(['row_id'],axis = 1)\nfor i in range(300):\n    train_data[f'f_{i}'] = train_data[f'f_{i}'].astype('float32')\nprint('Ending Dataframe size: %2.2f GB'%(getsizeof(train_data)/(1024.0**3)))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:41:12.487172Z","iopub.execute_input":"2022-01-26T23:41:12.487417Z","iopub.status.idle":"2022-01-26T23:41:34.518440Z","shell.execute_reply.started":"2022-01-26T23:41:12.487386Z","shell.execute_reply":"2022-01-26T23:41:34.517804Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:41:34.519572Z","iopub.execute_input":"2022-01-26T23:41:34.519822Z","iopub.status.idle":"2022-01-26T23:41:36.969908Z","shell.execute_reply.started":"2022-01-26T23:41:34.519793Z","shell.execute_reply":"2022-01-26T23:41:36.969081Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"target = train_data['target']\ntrain_data = train_data.drop(['target'],axis = 1)\nfrom sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(train_data, target, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:41:36.971092Z","iopub.execute_input":"2022-01-26T23:41:36.971712Z","iopub.status.idle":"2022-01-26T23:41:38.388476Z","shell.execute_reply.started":"2022-01-26T23:41:36.971672Z","shell.execute_reply":"2022-01-26T23:41:38.387580Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense,Dropout","metadata":{"execution":{"iopub.status.busy":"2022-01-27T00:21:46.803794Z","iopub.execute_input":"2022-01-27T00:21:46.804133Z","iopub.status.idle":"2022-01-27T00:21:46.810090Z","shell.execute_reply.started":"2022-01-27T00:21:46.804099Z","shell.execute_reply":"2022-01-27T00:21:46.808668Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def X_norm(X):   \n        mean = np.average(X_train, axis=0)\n        sigma = np.var(X_train, axis=0)**0.5\n        X_new = (X - mean )/sigma\n        return X_new\n    \ndef y_norm(y):\n        mean = np.average(y_train, axis=0)\n        sigma = np.var(y_train, axis=0)**0.5\n        y_new = (y - mean )/sigma\n        return y_new","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:41:44.657246Z","iopub.execute_input":"2022-01-26T23:41:44.657608Z","iopub.status.idle":"2022-01-26T23:41:44.666193Z","shell.execute_reply.started":"2022-01-26T23:41:44.657561Z","shell.execute_reply":"2022-01-26T23:41:44.665512Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_scaled_train = scaler.transform(X_train)\nX_scaled_val = scaler.transform(X_val)\nX_norm_train = X_norm(X_train)\nX_norm_val = X_norm(X_val)\ny_norm_train = y_norm(y_train)\ny_norm_val = y_norm(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T23:45:34.450488Z","iopub.execute_input":"2022-01-26T23:45:34.451502Z","iopub.status.idle":"2022-01-26T23:45:35.890052Z","shell.execute_reply.started":"2022-01-26T23:45:34.451455Z","shell.execute_reply":"2022-01-26T23:45:35.889010Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.2))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.2))\n    model.add(Dense(100, activation=act))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.01,)    \n    model.compile(loss ='mean_squared_error')\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T00:22:16.943397Z","iopub.execute_input":"2022-01-27T00:22:16.943727Z","iopub.status.idle":"2022-01-27T00:40:08.028877Z","shell.execute_reply.started":"2022-01-27T00:22:16.943692Z","shell.execute_reply":"2022-01-27T00:40:08.027448Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=act))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.01,)    \n    model.compile(loss ='mean_squared_error')\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T00:50:55.200382Z","iopub.execute_input":"2022-01-27T00:50:55.200803Z","iopub.status.idle":"2022-01-27T01:05:41.806822Z","shell.execute_reply.started":"2022-01-27T00:50:55.200756Z","shell.execute_reply":"2022-01-27T01:05:41.805913Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.01,)    \n    model.compile(loss ='mean_squared_error')\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T01:19:37.778211Z","iopub.execute_input":"2022-01-27T01:19:37.778492Z","iopub.status.idle":"2022-01-27T01:31:19.542066Z","shell.execute_reply.started":"2022-01-27T01:19:37.778461Z","shell.execute_reply":"2022-01-27T01:31:19.541132Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=act))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.001,)    \n    model.compile(loss ='mean_squared_error',metrics=[tf.keras.metrics.MeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsoluteError(),\n                                                     tf.keras.metrics.RootMeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsolutePercentageError(),\n                                                     tf.keras.metrics.MeanSquaredLogarithmicError(),])\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T01:32:01.016384Z","iopub.execute_input":"2022-01-27T01:32:01.017238Z","iopub.status.idle":"2022-01-27T01:44:00.965662Z","shell.execute_reply.started":"2022-01-27T01:32:01.017202Z","shell.execute_reply":"2022-01-27T01:44:00.964823Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=act))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.0001,)    \n    model.compile(loss ='mean_squared_error',metrics=[tf.keras.metrics.MeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsoluteError(),\n                                                     tf.keras.metrics.RootMeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsolutePercentageError(),\n                                                     tf.keras.metrics.MeanSquaredLogarithmicError(),])\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T01:44:08.063369Z","iopub.execute_input":"2022-01-27T01:44:08.063851Z","iopub.status.idle":"2022-01-27T01:55:59.925697Z","shell.execute_reply.started":"2022-01-27T01:44:08.063808Z","shell.execute_reply":"2022-01-27T01:55:59.924907Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nModels = {}\nactivation = ['tanh','elu']\nModels['activation'] = {}\n\nfor act in activation:\n    \n    np.random.seed(42)\n    model = keras.Sequential()\n    model.add(Dense(1000, input_shape=(X_norm_train.shape[1],), activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000, activation=act))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=act))\n    model.add(Dense(1, activation=act))\n\n    opt = Adam(learning_rate= 0.00001,)    \n    model.compile(loss ='mean_squared_error',metrics=[tf.keras.metrics.MeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsoluteError(),\n                                                     tf.keras.metrics.RootMeanSquaredError(),\n                                                     tf.keras.metrics.MeanAbsolutePercentageError(),\n                                                     tf.keras.metrics.MeanSquaredLogarithmicError(),])\n    history = model.fit(x = X_norm_train, y = y_train, epochs=10, verbose = 1, validation_data=(X_norm_val, y_val))\n    \n    Models['activation'][act] = model\n    \n    print('Train set min loss: {}'.format(min(history.history['loss'])))\n    print('Val set min loss: {}'.format(min(history.history['val_loss'])))    \n    plt.figure()\n    plt.title(act)\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    \n    tf.keras.backend.clear_session()\n    del model\n    ","metadata":{},"execution_count":null,"outputs":[]}]}