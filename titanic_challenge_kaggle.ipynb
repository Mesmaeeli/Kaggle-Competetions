{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nfrom scipy import stats # for outlier finding\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visalisation\nimport seaborn as sns # ata visalisation\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T17:42:00.464957Z","iopub.execute_input":"2022-01-25T17:42:00.465561Z","iopub.status.idle":"2022-01-25T17:42:01.708638Z","shell.execute_reply.started":"2022-01-25T17:42:00.465454Z","shell.execute_reply":"2022-01-25T17:42:01.707073Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## A little data investigation:\n* Finding outliers\n* Finding missing values\n* Finding correlated data\n","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data  = pd.read_csv('../input/titanic/test.csv')\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.710411Z","iopub.execute_input":"2022-01-25T17:42:01.710697Z","iopub.status.idle":"2022-01-25T17:42:01.784282Z","shell.execute_reply.started":"2022-01-25T17:42:01.710671Z","shell.execute_reply":"2022-01-25T17:42:01.783086Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"There are 891 records and 12 columns.\nWe would now count extract each column type, some statistical operations, and ","metadata":{}},{"cell_type":"code","source":"train_data.describe()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.785679Z","iopub.execute_input":"2022-01-25T17:42:01.785974Z","iopub.status.idle":"2022-01-25T17:42:01.822177Z","shell.execute_reply.started":"2022-01-25T17:42:01.785946Z","shell.execute_reply":"2022-01-25T17:42:01.820943Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.823585Z","iopub.execute_input":"2022-01-25T17:42:01.823896Z","iopub.status.idle":"2022-01-25T17:42:01.832400Z","shell.execute_reply.started":"2022-01-25T17:42:01.823869Z","shell.execute_reply":"2022-01-25T17:42:01.831073Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print('Percentage of NaN cells:')\nprint(train_data.isna().sum()/len(train_data)*100)\nprint('---')\nprint('Percentage of Null cells:')\nprint(train_data.isnull().sum()/len(train_data)*100)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.836415Z","iopub.execute_input":"2022-01-25T17:42:01.837076Z","iopub.status.idle":"2022-01-25T17:42:01.865473Z","shell.execute_reply.started":"2022-01-25T17:42:01.837019Z","shell.execute_reply":"2022-01-25T17:42:01.864538Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"* As we can see there are lots of missing values for Cabin column. \n* So we would simply drop it, as filling it with mean is not a logical approach!\n* We can fill missing age values with mean of the column and fill lost embarked values with mode of the column (Since this column is categorical).\n* Also PassengerID and Name are not useful columns for our dataset to detect whether the passenger has survived or not based on these columns. So, we would drop them too.\n","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['Cabin'],axis = 1)\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode())\ntrain_data = train_data.drop(['PassengerId','Name'],axis = 1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.866882Z","iopub.execute_input":"2022-01-25T17:42:01.867351Z","iopub.status.idle":"2022-01-25T17:42:01.899353Z","shell.execute_reply.started":"2022-01-25T17:42:01.867319Z","shell.execute_reply":"2022-01-25T17:42:01.898371Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"* In this stage wefillnad have some data transformation. We would use one-hot encoding for Sex, and Embarked since the are categorical. \n* We also need some transformation for Tickets, since the are a combination of charachters and digits. We would just pick the numerical value of the Ticket. Other columns are just fine for this stage.","metadata":{}},{"cell_type":"code","source":"for index,row in train_data.iterrows():\n    value = str(row['Ticket']).split()[-1]\n    if value.isdigit():\n        train_data.at[index,'Ticket'] = int(str(row['Ticket']).split()[-1])\n    else:\n        train_data.at[index,'Ticket'] = 0\n        \ntrain_data['Ticket'] = train_data['Ticket'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:01.900603Z","iopub.execute_input":"2022-01-25T17:42:01.900887Z","iopub.status.idle":"2022-01-25T17:42:02.007460Z","shell.execute_reply.started":"2022-01-25T17:42:01.900858Z","shell.execute_reply":"2022-01-25T17:42:02.006398Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# train_data['Ticket'] = train_data['Ticket'].map(lambda x: (str(x).split()[-1])).replace(',', '').replace('\\n', '').astype(int)\n# # train_data['Ticket'] = train_data['Ticket'].astype('string')\n# # train_data['Ticket'] = a.astype('string')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:02.008945Z","iopub.execute_input":"2022-01-25T17:42:02.009379Z","iopub.status.idle":"2022-01-25T17:42:02.013163Z","shell.execute_reply.started":"2022-01-25T17:42:02.009338Z","shell.execute_reply":"2022-01-25T17:42:02.012449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# def int_taker(arg):\n#     try:\n#         return int(float(arg.split()[1]))\n#     except:\n#         return int(float(arg.split()[0]))\n\n# # train_data['Ticket'] = train_data['Ticket'].map(lambda x: int_taker(str(x)))\n\ntrain_data = pd.get_dummies(train_data, columns=['Sex','Embarked'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:02.014161Z","iopub.execute_input":"2022-01-25T17:42:02.014621Z","iopub.status.idle":"2022-01-25T17:42:02.033955Z","shell.execute_reply.started":"2022-01-25T17:42:02.014592Z","shell.execute_reply":"2022-01-25T17:42:02.032937Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Now we would try to find outliers based on z-score for each record.","metadata":{}},{"cell_type":"code","source":"\n\n# firts we would drop 'Suvived' as this is our target, and save it with variable name of Target\n\ntarget = train_data['Survived']\ntrain_data = train_data.drop(['Survived'],axis = 1)\n\ncolumns = train_data.select_dtypes(include=np.number).columns\n\nz_score = {}\nfor col in columns:\n    z_score[col] = np.abs(stats.zscore(train_data[col]))\n# z_score_df = pd.DataFrame(z_score, columns = columns)\n\nz_score_df = pd.DataFrame(z_score, columns = columns)\nz_score_df","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:02.035491Z","iopub.execute_input":"2022-01-25T17:42:02.036170Z","iopub.status.idle":"2022-01-25T17:42:02.070453Z","shell.execute_reply.started":"2022-01-25T17:42:02.036125Z","shell.execute_reply":"2022-01-25T17:42:02.069319Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for feature in z_score_df.columns:\n    fig, ax = plt.subplots()\n    z_score_df[feature].plot(kind = 'kde')\n    ax.set_title(feature)\n    \n    quant_5, quant_25, quant_50, quant_75, quant_95 = z_score_df[feature].quantile(0.05), z_score_df[feature].quantile(0.25),z_score_df[feature].quantile(0.5), z_score_df[feature].quantile(0.75), z_score_df[feature].quantile(0.95)\n    quants = [[quant_5, 0.6, 'r'], [quant_25, 0.8, 'g'], [quant_50, 1, 'b'],  [quant_75, 0.8, 'm'], [quant_95, 0.6,'k']]\n    for i in quants:\n        ax.axvline(i[0],alpha = i[1], linestyle = \":\",color = i[2])\n    ax.set_xlim(xmin = -1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:02.071902Z","iopub.execute_input":"2022-01-25T17:42:02.072452Z","iopub.status.idle":"2022-01-25T17:42:04.754193Z","shell.execute_reply.started":"2022-01-25T17:42:02.072405Z","shell.execute_reply":"2022-01-25T17:42:04.753024Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* As we can see all values are in a reasonable range of z-scores, so we can assume that there is no outliers in our dataset.\n* Now we would see is there is any unusual correlation between any two pairs of columns on our dataset.\n","metadata":{}},{"cell_type":"code","source":"cor_mat = train_data.corr(method= 'pearson')\nfig = plt.figure(figsize=(10,10))\nsns.heatmap(cor_mat)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:04.755700Z","iopub.execute_input":"2022-01-25T17:42:04.756088Z","iopub.status.idle":"2022-01-25T17:42:05.269333Z","shell.execute_reply.started":"2022-01-25T17:42:04.756048Z","shell.execute_reply":"2022-01-25T17:42:05.268365Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* as we expected, there is a high negative correlation between sex_male and sex_female, so we should just keep one.\n* Also we can keep two of embarments since keeping all three is not reasonable.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['Embarked_C'],axis = 1)\ntrain_data = train_data.drop(['Sex_male'],axis = 1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:05.270533Z","iopub.execute_input":"2022-01-25T17:42:05.270849Z","iopub.status.idle":"2022-01-25T17:42:05.294627Z","shell.execute_reply.started":"2022-01-25T17:42:05.270822Z","shell.execute_reply":"2022-01-25T17:42:05.293814Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Now that data is cleaned and checked, we can use our models to make predictions.","metadata":{}},{"cell_type":"markdown","source":"## Classification metrics function\n\n* we will make a function for our models to examine how it is performing.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\n\n\ndef metrics(model, xtest, ytest):\n    print('F1 Score:',f1_score(xtest,ytest))\n    print('Weighted F1 Score:',fbeta_score(xtest,ytest))\n    print('Log Loss:',log_loss(xtest,ytest))\n    print('AUC Score:',roc_auc_score(xtest,ytest))\n    print('Recall Score:',recall_score(xtest,ytest))\n    print('Precision Score:',precision_score(xtest,ytest))\n    print('Accuracy Score:',accuracy_score(xtest,ytest))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:05.295986Z","iopub.execute_input":"2022-01-25T17:42:05.296385Z","iopub.status.idle":"2022-01-25T17:42:05.303375Z","shell.execute_reply.started":"2022-01-25T17:42:05.296354Z","shell.execute_reply":"2022-01-25T17:42:05.302632Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n* Train the model based on a linear regression approach","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_data, target, test_size=0.3, random_state=42)\n\nmodel1 = LogisticRegression(random_state=0).fit(X_train, y_train)\nmodel1.score(X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:27.778933Z","iopub.execute_input":"2022-01-25T17:42:27.779425Z","iopub.status.idle":"2022-01-25T17:42:27.800956Z","shell.execute_reply.started":"2022-01-25T17:42:27.779394Z","shell.execute_reply":"2022-01-25T17:42:27.800104Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"* Not an acceptable result! We try to make some adjustments to see if we can increase the accuracy.\n* First we check for feature scaling:","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_scaled_train = scaler.transform(X_train)\nmodel2 = LogisticRegression(random_state=0).fit(X_scaled_train, y_train)\nX_scaled_val = scaler.transform(X_val)\nmodel2.score(X_scaled_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:39.311243Z","iopub.execute_input":"2022-01-25T17:42:39.311666Z","iopub.status.idle":"2022-01-25T17:42:39.334780Z","shell.execute_reply.started":"2022-01-25T17:42:39.311629Z","shell.execute_reply":"2022-01-25T17:42:39.333663Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"* A very satisfying increase in our accuracy. Now we would check if our data is imbalanced. If it was imbalanced we would use some weights to tackle tis situation.","metadata":{}},{"cell_type":"code","source":"print('Number of 1s in train set target:',sum(y_train))\nprint('total train number:',len(y_train))\nprint('Number of 1s in validation set target:',sum(y_val))\nprint('total validation number:',len(y_val))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:37:26.628407Z","iopub.execute_input":"2022-01-25T17:37:26.629159Z","iopub.status.idle":"2022-01-25T17:37:26.636766Z","shell.execute_reply.started":"2022-01-25T17:37:26.629109Z","shell.execute_reply":"2022-01-25T17:37:26.635391Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"* The data is slightly imbalanced, so we would use weights for our model","metadata":{}},{"cell_type":"code","source":"model3 = LogisticRegression(random_state=0, class_weight=\"balanced\").fit(X_scaled_train, y_train)\nmodel3.score(X_scaled_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:42:59.918731Z","iopub.execute_input":"2022-01-25T17:42:59.919092Z","iopub.status.idle":"2022-01-25T17:42:59.936331Z","shell.execute_reply.started":"2022-01-25T17:42:59.919059Z","shell.execute_reply":"2022-01-25T17:42:59.935351Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"* So this effort was not a good idea. lets check all parameters that sklearn suggest for logistic regression:","metadata":{}},{"cell_type":"code","source":"model4 = LogisticRegression(random_state=0, penalty = 'none').fit(X_scaled_train, y_train)\nprint('Model with no penalty term:')\nprint(model4.score(X_scaled_val, y_val),'\\n')\n\nmodel5 = LogisticRegression(random_state=0, fit_intercept = False).fit(X_scaled_train, y_train)\nprint('Model with no interception fit:')\nprint(model5.score(X_scaled_val, y_val),'\\n')\n\nmodel6 = LogisticRegression(random_state=0, solver = 'newton-cg').fit(X_scaled_train, y_train)\nprint('Model with newton-cg solver')\nprint(model6.score(X_scaled_val, y_val),'\\n')\n\nmodel7 = LogisticRegression(random_state=0, solver = 'liblinear').fit(X_scaled_train, y_train)\nprint('Model with liblinear solver')\nprint(model7.score(X_scaled_val, y_val),'\\n')\n\nmodel8 = LogisticRegression(random_state=0, solver = 'sag').fit(X_scaled_train, y_train)\nprint('Model with sag solver')\nprint(model8.score(X_scaled_val, y_val),'\\n')\n\nmodel9 = LogisticRegression(random_state=0, solver = 'saga').fit(X_scaled_train, y_train)\nprint('Model with saga solver')\nprint(model9.score(X_scaled_val, y_val),'\\n')\n\nmodel10 = LogisticRegression(random_state=0, max_iter = 200).fit(X_scaled_train, y_train)\nprint('Model with 200 iterations:')\nprint(model10.score(X_scaled_val, y_val),'\\n')\n\nmodel11 = LogisticRegression(random_state=0, multi_class = 'ovr').fit(X_scaled_train, y_train)\nprint('Model with binary classification:')\nprint(model11.score(X_scaled_val, y_val),'\\n')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T17:56:01.437342Z","iopub.execute_input":"2022-01-25T17:56:01.437965Z","iopub.status.idle":"2022-01-25T17:56:01.505256Z","shell.execute_reply.started":"2022-01-25T17:56:01.437904Z","shell.execute_reply":"2022-01-25T17:56:01.504471Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"* So that was all we could do with logistic regression as other extra parameters didn't help us at all. So we would use model2.\n* lets see the model's performance on test dataset. first we would do some preprocessing for the test data (removing Name, transforming Ticket, ...)","metadata":{}},{"cell_type":"code","source":"print('Percentage of NaN cells:')\nprint(test_data.isna().sum()/len(test_data)*100)\nprint('---')\nprint('Percentage of Null cells:')\nprint(test_data.isnull().sum()/len(test_data)*100)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T18:15:21.960580Z","iopub.execute_input":"2022-01-25T18:15:21.960948Z","iopub.status.idle":"2022-01-25T18:15:21.974438Z","shell.execute_reply.started":"2022-01-25T18:15:21.960916Z","shell.execute_reply":"2022-01-25T18:15:21.973632Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_data  = pd.read_csv('../input/titanic/test.csv')\n\ntest_data = test_data.drop(['Cabin'],axis = 1)\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())\ntest_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode())\np_id = np.hstack((test_data.PassengerId.to_numpy().reshape(-1,1)))\ntest_data = test_data.drop(['PassengerId','Name'],axis = 1)\n\n\nfor index,row in test_data.iterrows():\n    value = str(row['Ticket']).split()[-1]\n    if value.isdigit():\n        test_data.at[index,'Ticket'] = int(str(row['Ticket']).split()[-1])\n    else:\n        test_data.at[index,'Ticket'] = 0\n        \ntest_data['Ticket'] = test_data['Ticket'].astype('int')\n\ntest_data = pd.get_dummies(test_data, columns=['Sex','Embarked'])\n\ntest_data = test_data.drop(['Embarked_C'],axis = 1)\nX_test = test_data.drop(['Sex_male'],axis = 1)\n\nX_scaled_test = scaler.transform(X_test)\ny_hat = model2.predict(X_scaled_test)\n\ny_hat = y_hat.reshape(-1,1)\nresult = np.hstack((p_id.reshape(-1,1),y_hat))\ndf = pd.DataFrame(result, columns = ['PassengerId', 'Survived'])\ndf.to_csv('submission.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-25T18:34:49.180465Z","iopub.execute_input":"2022-01-25T18:34:49.180828Z","iopub.status.idle":"2022-01-25T18:34:49.274648Z","shell.execute_reply.started":"2022-01-25T18:34:49.180796Z","shell.execute_reply":"2022-01-25T18:34:49.273548Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Final accuracy for logistic regression model: 0.77033\n\n* lets try ANN to see if there is any improvements","metadata":{}},{"cell_type":"markdown","source":"## Artificial Neural Network\n* Train the model with a simple neural network (2 Hidden Layers)\n* Train the model with deep neural netwok","metadata":{}}]}