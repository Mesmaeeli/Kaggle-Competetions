{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nfrom scipy import stats # for outlier finding\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visalisation\nimport seaborn as sns # ata visalisation\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T22:53:24.861714Z","iopub.execute_input":"2022-01-25T22:53:24.862145Z","iopub.status.idle":"2022-01-25T22:53:26.078904Z","shell.execute_reply.started":"2022-01-25T22:53:24.862049Z","shell.execute_reply":"2022-01-25T22:53:26.078096Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## A little data investigation:\n* Finding outliers\n* Finding missing values\n* Finding correlated data\n","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data  = pd.read_csv('../input/titanic/test.csv')\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.080086Z","iopub.execute_input":"2022-01-25T22:53:26.080384Z","iopub.status.idle":"2022-01-25T22:53:26.145057Z","shell.execute_reply.started":"2022-01-25T22:53:26.080360Z","shell.execute_reply":"2022-01-25T22:53:26.144056Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"There are 891 records and 12 columns.\nWe would now count extract each column type, some statistical operations, and ","metadata":{}},{"cell_type":"code","source":"train_data.describe()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.146318Z","iopub.execute_input":"2022-01-25T22:53:26.146737Z","iopub.status.idle":"2022-01-25T22:53:26.183975Z","shell.execute_reply.started":"2022-01-25T22:53:26.146701Z","shell.execute_reply":"2022-01-25T22:53:26.183010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.185189Z","iopub.execute_input":"2022-01-25T22:53:26.185497Z","iopub.status.idle":"2022-01-25T22:53:26.194938Z","shell.execute_reply.started":"2022-01-25T22:53:26.185464Z","shell.execute_reply":"2022-01-25T22:53:26.193377Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print('Percentage of NaN cells:')\nprint(train_data.isna().sum()/len(train_data)*100)\nprint('---')\nprint('Percentage of Null cells:')\nprint(train_data.isnull().sum()/len(train_data)*100)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.197621Z","iopub.execute_input":"2022-01-25T22:53:26.198050Z","iopub.status.idle":"2022-01-25T22:53:26.230540Z","shell.execute_reply.started":"2022-01-25T22:53:26.198014Z","shell.execute_reply":"2022-01-25T22:53:26.229654Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"* As we can see there are lots of missing values for Cabin column. \n* So we would simply drop it, as filling it with mean is not a logical approach!\n* We can fill missing age values with mean of the column and fill lost embarked values with mode of the column (Since this column is categorical).\n* Also PassengerID and Name are not useful columns for our dataset to detect whether the passenger has survived or not based on these columns. So, we would drop them too.\n","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['Cabin'],axis = 1)\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode())\ntrain_data = train_data.drop(['PassengerId','Name'],axis = 1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.234644Z","iopub.execute_input":"2022-01-25T22:53:26.235920Z","iopub.status.idle":"2022-01-25T22:53:26.272290Z","shell.execute_reply.started":"2022-01-25T22:53:26.235821Z","shell.execute_reply":"2022-01-25T22:53:26.270798Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"* In this stage wefillnad have some data transformation. We would use one-hot encoding for Sex, and Embarked since the are categorical. \n* We also need some transformation for Tickets, since the are a combination of charachters and digits. We would just pick the numerical value of the Ticket. Other columns are just fine for this stage.","metadata":{}},{"cell_type":"code","source":"for index,row in train_data.iterrows():\n    value = str(row['Ticket']).split()[-1]\n    if value.isdigit():\n        train_data.at[index,'Ticket'] = int(str(row['Ticket']).split()[-1])\n    else:\n        train_data.at[index,'Ticket'] = 0\n        \ntrain_data['Ticket'] = train_data['Ticket'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.274535Z","iopub.execute_input":"2022-01-25T22:53:26.274848Z","iopub.status.idle":"2022-01-25T22:53:26.378874Z","shell.execute_reply.started":"2022-01-25T22:53:26.274822Z","shell.execute_reply":"2022-01-25T22:53:26.377725Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# train_data['Ticket'] = train_data['Ticket'].map(lambda x: (str(x).split()[-1])).replace(',', '').replace('\\n', '').astype(int)\n# # train_data['Ticket'] = train_data['Ticket'].astype('string')\n# # train_data['Ticket'] = a.astype('string')","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.380234Z","iopub.execute_input":"2022-01-25T22:53:26.380526Z","iopub.status.idle":"2022-01-25T22:53:26.384687Z","shell.execute_reply.started":"2022-01-25T22:53:26.380497Z","shell.execute_reply":"2022-01-25T22:53:26.383635Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# def int_taker(arg):\n#     try:\n#         return int(float(arg.split()[1]))\n#     except:\n#         return int(float(arg.split()[0]))\n\n# # train_data['Ticket'] = train_data['Ticket'].map(lambda x: int_taker(str(x)))\n\ntrain_data = pd.get_dummies(train_data, columns=['Sex','Embarked'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.385774Z","iopub.execute_input":"2022-01-25T22:53:26.385985Z","iopub.status.idle":"2022-01-25T22:53:26.407206Z","shell.execute_reply.started":"2022-01-25T22:53:26.385964Z","shell.execute_reply":"2022-01-25T22:53:26.406467Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Now we would try to find outliers based on z-score for each record.","metadata":{}},{"cell_type":"code","source":"\n\n# firts we would drop 'Suvived' as this is our target, and save it with variable name of Target\n\ntarget = train_data['Survived']\ntrain_data = train_data.drop(['Survived'],axis = 1)\n\ncolumns = train_data.select_dtypes(include=np.number).columns\n\nz_score = {}\nfor col in columns:\n    z_score[col] = np.abs(stats.zscore(train_data[col]))\n# z_score_df = pd.DataFrame(z_score, columns = columns)\n\nz_score_df = pd.DataFrame(z_score, columns = columns)\nz_score_df","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.408193Z","iopub.execute_input":"2022-01-25T22:53:26.408522Z","iopub.status.idle":"2022-01-25T22:53:26.434967Z","shell.execute_reply.started":"2022-01-25T22:53:26.408489Z","shell.execute_reply":"2022-01-25T22:53:26.434146Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for feature in z_score_df.columns:\n    fig, ax = plt.subplots()\n    z_score_df[feature].plot(kind = 'kde')\n    ax.set_title(feature)\n    \n    quant_5, quant_25, quant_50, quant_75, quant_95 = z_score_df[feature].quantile(0.05), z_score_df[feature].quantile(0.25),z_score_df[feature].quantile(0.5), z_score_df[feature].quantile(0.75), z_score_df[feature].quantile(0.95)\n    quants = [[quant_5, 0.6, 'r'], [quant_25, 0.8, 'g'], [quant_50, 1, 'b'],  [quant_75, 0.8, 'm'], [quant_95, 0.6,'k']]\n    for i in quants:\n        ax.axvline(i[0],alpha = i[1], linestyle = \":\",color = i[2])\n    ax.set_xlim(xmin = -1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:26.435942Z","iopub.execute_input":"2022-01-25T22:53:26.436197Z","iopub.status.idle":"2022-01-25T22:53:28.531625Z","shell.execute_reply.started":"2022-01-25T22:53:26.436175Z","shell.execute_reply":"2022-01-25T22:53:28.530898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* As we can see all values are in a reasonable range of z-scores, so we can assume that there is no outliers in our dataset.\n* Now we would see is there is any unusual correlation between any two pairs of columns on our dataset.\n","metadata":{}},{"cell_type":"code","source":"cor_mat = train_data.corr(method= 'pearson')\nfig = plt.figure(figsize=(10,10))\nsns.heatmap(cor_mat)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:28.533161Z","iopub.execute_input":"2022-01-25T22:53:28.533444Z","iopub.status.idle":"2022-01-25T22:53:28.969186Z","shell.execute_reply.started":"2022-01-25T22:53:28.533415Z","shell.execute_reply":"2022-01-25T22:53:28.968129Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* as we expected, there is a high negative correlation between sex_male and sex_female, so we should just keep one.\n* Also we can keep two of embarments since keeping all three is not reasonable.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['Embarked_C'],axis = 1)\ntrain_data = train_data.drop(['Sex_male'],axis = 1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:28.970912Z","iopub.execute_input":"2022-01-25T22:53:28.971335Z","iopub.status.idle":"2022-01-25T22:53:28.995073Z","shell.execute_reply.started":"2022-01-25T22:53:28.971308Z","shell.execute_reply":"2022-01-25T22:53:28.993830Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Now that data is cleaned and checked, we can use our models to make predictions.","metadata":{}},{"cell_type":"markdown","source":"## Classification metrics function\n\n* we will make a function for our models to examine how it is performing.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\n\n\ndef metrics(model, xtest, ytest):\n    print('F1 Score:',f1_score(xtest,ytest))\n    print('Weighted F1 Score:',fbeta_score(xtest,ytest))\n    print('Log Loss:',log_loss(xtest,ytest))\n    print('AUC Score:',roc_auc_score(xtest,ytest))\n    print('Recall Score:',recall_score(xtest,ytest))\n    print('Precision Score:',precision_score(xtest,ytest))\n    print('Accuracy Score:',accuracy_score(xtest,ytest))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:28.996750Z","iopub.execute_input":"2022-01-25T22:53:28.997160Z","iopub.status.idle":"2022-01-25T22:53:29.007726Z","shell.execute_reply.started":"2022-01-25T22:53:28.997118Z","shell.execute_reply":"2022-01-25T22:53:29.006348Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n* Train the model based on a linear regression approach","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_data, target, test_size=0.3, random_state=42)\n\nmodel1 = LogisticRegression(random_state=0).fit(X_train, y_train)\nmodel1.score(X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.009687Z","iopub.execute_input":"2022-01-25T22:53:29.010061Z","iopub.status.idle":"2022-01-25T22:53:29.037330Z","shell.execute_reply.started":"2022-01-25T22:53:29.010028Z","shell.execute_reply":"2022-01-25T22:53:29.036622Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"* Not an acceptable result! We try to make some adjustments to see if we can increase the accuracy.\n* First we check for feature scaling:","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_scaled_train = scaler.transform(X_train)\nmodel2 = LogisticRegression(random_state=0).fit(X_scaled_train, y_train)\nX_scaled_val = scaler.transform(X_val)\nmodel2.score(X_scaled_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.038356Z","iopub.execute_input":"2022-01-25T22:53:29.038660Z","iopub.status.idle":"2022-01-25T22:53:29.060315Z","shell.execute_reply.started":"2022-01-25T22:53:29.038631Z","shell.execute_reply":"2022-01-25T22:53:29.058779Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"* A very satisfying increase in our accuracy. Now we would check if our data is imbalanced. If it was imbalanced we would use some weights to tackle tis situation.","metadata":{}},{"cell_type":"code","source":"print('Number of 1s in train set target:',sum(y_train))\nprint('total train number:',len(y_train))\nprint('Number of 1s in validation set target:',sum(y_val))\nprint('total validation number:',len(y_val))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.061965Z","iopub.execute_input":"2022-01-25T22:53:29.062297Z","iopub.status.idle":"2022-01-25T22:53:29.070185Z","shell.execute_reply.started":"2022-01-25T22:53:29.062264Z","shell.execute_reply":"2022-01-25T22:53:29.068941Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"* The data is slightly imbalanced, so we would use weights for our model","metadata":{}},{"cell_type":"code","source":"model3 = LogisticRegression(random_state=0, class_weight=\"balanced\").fit(X_scaled_train, y_train)\nmodel3.score(X_scaled_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.071630Z","iopub.execute_input":"2022-01-25T22:53:29.072006Z","iopub.status.idle":"2022-01-25T22:53:29.092977Z","shell.execute_reply.started":"2022-01-25T22:53:29.071964Z","shell.execute_reply":"2022-01-25T22:53:29.090941Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"* So this effort was not a good idea. lets check all parameters that sklearn suggest for logistic regression:","metadata":{}},{"cell_type":"code","source":"model4 = LogisticRegression(random_state=0, penalty = 'none').fit(X_scaled_train, y_train)\nprint('Model with no penalty term:')\nprint(model4.score(X_scaled_val, y_val),'\\n')\n\nmodel5 = LogisticRegression(random_state=0, fit_intercept = False).fit(X_scaled_train, y_train)\nprint('Model with no interception fit:')\nprint(model5.score(X_scaled_val, y_val),'\\n')\n\nmodel6 = LogisticRegression(random_state=0, solver = 'newton-cg').fit(X_scaled_train, y_train)\nprint('Model with newton-cg solver')\nprint(model6.score(X_scaled_val, y_val),'\\n')\n\nmodel7 = LogisticRegression(random_state=0, solver = 'liblinear').fit(X_scaled_train, y_train)\nprint('Model with liblinear solver')\nprint(model7.score(X_scaled_val, y_val),'\\n')\n\nmodel8 = LogisticRegression(random_state=0, solver = 'sag').fit(X_scaled_train, y_train)\nprint('Model with sag solver')\nprint(model8.score(X_scaled_val, y_val),'\\n')\n\nmodel9 = LogisticRegression(random_state=0, solver = 'saga').fit(X_scaled_train, y_train)\nprint('Model with saga solver')\nprint(model9.score(X_scaled_val, y_val),'\\n')\n\nmodel10 = LogisticRegression(random_state=0, max_iter = 200).fit(X_scaled_train, y_train)\nprint('Model with 200 iterations:')\nprint(model10.score(X_scaled_val, y_val),'\\n')\n\nmodel11 = LogisticRegression(random_state=0, multi_class = 'ovr').fit(X_scaled_train, y_train)\nprint('Model with binary classification:')\nprint(model11.score(X_scaled_val, y_val),'\\n')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.094493Z","iopub.execute_input":"2022-01-25T22:53:29.094837Z","iopub.status.idle":"2022-01-25T22:53:29.168689Z","shell.execute_reply.started":"2022-01-25T22:53:29.094807Z","shell.execute_reply":"2022-01-25T22:53:29.167479Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"* So that was all we could do with logistic regression as other extra parameters didn't help us at all. So we would use model2.\n* lets see the model's performance on test dataset. first we would do some preprocessing for the test data (removing Name, transforming Ticket, ...)","metadata":{}},{"cell_type":"code","source":"print('Percentage of NaN cells:')\nprint(test_data.isna().sum()/len(test_data)*100)\nprint('---')\nprint('Percentage of Null cells:')\nprint(test_data.isnull().sum()/len(test_data)*100)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.170102Z","iopub.execute_input":"2022-01-25T22:53:29.170443Z","iopub.status.idle":"2022-01-25T22:53:29.182680Z","shell.execute_reply.started":"2022-01-25T22:53:29.170409Z","shell.execute_reply":"2022-01-25T22:53:29.181849Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_data  = pd.read_csv('../input/titanic/test.csv')\n\ntest_data = test_data.drop(['Cabin'],axis = 1)\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())\ntest_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode())\np_id = np.hstack((test_data.PassengerId.to_numpy().reshape(-1,1)))\ntest_data = test_data.drop(['PassengerId','Name'],axis = 1)\n\n\nfor index,row in test_data.iterrows():\n    value = str(row['Ticket']).split()[-1]\n    if value.isdigit():\n        test_data.at[index,'Ticket'] = int(str(row['Ticket']).split()[-1])\n    else:\n        test_data.at[index,'Ticket'] = 0\n        \ntest_data['Ticket'] = test_data['Ticket'].astype('int')\n\ntest_data = pd.get_dummies(test_data, columns=['Sex','Embarked'])\n\ntest_data = test_data.drop(['Embarked_C'],axis = 1)\nX_test = test_data.drop(['Sex_male'],axis = 1)\n\nX_scaled_test = scaler.transform(X_test)\ny_hat = model2.predict(X_scaled_test)\n\ny_hat = y_hat.reshape(-1,1)\nresult = np.hstack((p_id.reshape(-1,1),y_hat))\ndf = pd.DataFrame(result, columns = ['PassengerId', 'Survived'])\ndf.to_csv('submission.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.183697Z","iopub.execute_input":"2022-01-25T22:53:29.183967Z","iopub.status.idle":"2022-01-25T22:53:29.271701Z","shell.execute_reply.started":"2022-01-25T22:53:29.183934Z","shell.execute_reply":"2022-01-25T22:53:29.270587Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Final accuracy for logistic regression model: 0.77033\n\n* lets try ANN to see if there is any improvements","metadata":{}},{"cell_type":"markdown","source":"## Artificial Neural Network\n* Train the model with a simple neural network (2 Hidden Layers)\n* Train the model with deep neural netwok","metadata":{}},{"cell_type":"code","source":"# First we would import some libraries that we will need for out ANN building\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:29.272916Z","iopub.execute_input":"2022-01-25T22:53:29.273335Z","iopub.status.idle":"2022-01-25T22:53:34.978070Z","shell.execute_reply.started":"2022-01-25T22:53:29.273303Z","shell.execute_reply":"2022-01-25T22:53:34.976900Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(Dense(10, input_shape=(9,), activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:34.979930Z","iopub.execute_input":"2022-01-25T22:53:34.980715Z","iopub.status.idle":"2022-01-25T22:53:35.079346Z","shell.execute_reply.started":"2022-01-25T22:53:34.980686Z","shell.execute_reply":"2022-01-25T22:53:35.078793Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"y_onehot_train = np.eye(2)[y_train.to_numpy().reshape(-1)]\ny_onehot_val = np.eye(2)[y_val.to_numpy().reshape(-1)]\n\nmodel.compile(loss = 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(),\n                                                     tf.keras.metrics.Accuracy(),\n                                                     tf.keras.metrics.BinaryAccuracy(),\n                                                     tf.keras.metrics.Recall(),\n                                                     tf.keras.metrics.Precision(),])\n\nhistory = model.fit(X_scaled_train, y_onehot_train, epochs=50,validation_data=(X_scaled_val, y_onehot_val))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:35.080251Z","iopub.execute_input":"2022-01-25T22:53:35.080589Z","iopub.status.idle":"2022-01-25T22:53:42.033037Z","shell.execute_reply.started":"2022-01-25T22:53:35.080533Z","shell.execute_reply":"2022-01-25T22:53:42.032058Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title('Cross Entropy Loss')\nplt.plot(history.history['loss'], label = 'Train')\nplt.plot(history.history['val_loss'], label = 'Validation')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:42.034950Z","iopub.execute_input":"2022-01-25T22:53:42.035277Z","iopub.status.idle":"2022-01-25T22:53:42.184197Z","shell.execute_reply.started":"2022-01-25T22:53:42.035242Z","shell.execute_reply":"2022-01-25T22:53:42.182777Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"* We try to optimize parameters to reach ","metadata":{}},{"cell_type":"code","source":"model1 = keras.Sequential()\nmodel1.add(Dense(10, input_shape=(9,), activation='relu'))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(2, activation='softmax'))\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:42.185496Z","iopub.execute_input":"2022-01-25T22:53:42.185776Z","iopub.status.idle":"2022-01-25T22:53:42.232990Z","shell.execute_reply.started":"2022-01-25T22:53:42.185749Z","shell.execute_reply":"2022-01-25T22:53:42.231646Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model1.compile(loss = 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(),\n                                                     tf.keras.metrics.Accuracy(),\n                                                     tf.keras.metrics.BinaryAccuracy(),\n                                                     tf.keras.metrics.Recall(),\n                                                     tf.keras.metrics.Precision(),])\n\nhistory = model1.fit(X_scaled_train, y_onehot_train, epochs=50,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\nplt.figure()\nplt.title('Cross Entropy Loss')\nplt.plot(history.history['loss'], label = 'Train')\nplt.plot(history.history['val_loss'], label = 'Validation')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:42.234127Z","iopub.execute_input":"2022-01-25T22:53:42.234482Z","iopub.status.idle":"2022-01-25T22:53:48.828349Z","shell.execute_reply.started":"2022-01-25T22:53:42.234446Z","shell.execute_reply":"2022-01-25T22:53:48.827419Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model2 = keras.Sequential()\nmodel2.add(Dense(10, input_shape=(9,), activation='relu'))\nmodel2.add(Dense(100, activation='relu'))\nmodel2.add(Dense(100, activation='relu'))\nmodel2.add(Dense(100, activation='relu'))\nmodel2.add(Dense(2, activation='softmax'))\nmodel2.summary()\n\nmodel2.compile(loss = 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(),\n                                                     tf.keras.metrics.Accuracy(),\n                                                     tf.keras.metrics.BinaryAccuracy(),\n                                                     tf.keras.metrics.Recall(),\n                                                     tf.keras.metrics.Precision(),])\n\nhistory = model2.fit(X_scaled_train, y_onehot_train, epochs=50,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\nplt.figure()\nplt.title('Cross Entropy Loss')\nplt.plot(history.history['loss'], label = 'Train')\nplt.plot(history.history['val_loss'], label = 'Validation')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:48.829611Z","iopub.execute_input":"2022-01-25T22:53:48.829893Z","iopub.status.idle":"2022-01-25T22:53:55.368887Z","shell.execute_reply.started":"2022-01-25T22:53:48.829859Z","shell.execute_reply":"2022-01-25T22:53:55.368296Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"* Adding more hidden layers are just making overfit. So will continue with model1 for parameter tuning.","metadata":{}},{"cell_type":"code","source":"for act in ['relu','sigmoid','tanh','elu','selu']:    \n    model1 = keras.Sequential()\n    model1.add(Dense(10, input_shape=(9,), activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(2, activation='softmax'))\n\n    model1.compile(loss = 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(),\n                                                         tf.keras.metrics.Accuracy(),\n                                                         tf.keras.metrics.BinaryAccuracy(),\n                                                         tf.keras.metrics.Recall(),\n                                                         tf.keras.metrics.Precision(),])\n\n    history = model1.fit(X_scaled_train, y_onehot_train, epochs=50,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\n    plt.figure()\n    plt.title('Activation Function: {}'.format(act))\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:53:55.369944Z","iopub.execute_input":"2022-01-25T22:53:55.370325Z","iopub.status.idle":"2022-01-25T22:54:27.836229Z","shell.execute_reply.started":"2022-01-25T22:53:55.370298Z","shell.execute_reply":"2022-01-25T22:54:27.835602Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"* lets continue with sogmoid with more iterations to see what happen:","metadata":{}},{"cell_type":"code","source":"act = 'sigmoid'\nmodel1 = keras.Sequential()\nmodel1.add(Dense(10, input_shape=(9,), activation= act))\nmodel1.add(Dense(100, activation= act))\nmodel1.add(Dense(100, activation= act))\nmodel1.add(Dense(2, activation='softmax'))\n\nmodel1.compile(loss = 'binary_crossentropy', metrics=[tf.keras.metrics.AUC(),\n                                                     tf.keras.metrics.Accuracy(),\n                                                     tf.keras.metrics.BinaryAccuracy(),\n                                                     tf.keras.metrics.Recall(),\n                                                     tf.keras.metrics.Precision(),])\n\nhistory = model1.fit(X_scaled_train, y_onehot_train, epochs=500,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\nplt.figure()\nplt.title('Activation Function: {}'.format(act))\nplt.plot(history.history['loss'], label = 'Train')\nplt.plot(history.history['val_loss'], label = 'Validation')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:55:31.889151Z","iopub.execute_input":"2022-01-25T22:55:31.889580Z","iopub.status.idle":"2022-01-25T22:56:21.565550Z","shell.execute_reply.started":"2022-01-25T22:55:31.889533Z","shell.execute_reply":"2022-01-25T22:56:21.563870Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"* now lets try using optimizers to see if they can help the performance","metadata":{}},{"cell_type":"code","source":"optimizers = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam']\nact = 'sigmoid'\n\nfor opt in optimizers:\n    tf.random.set_seed(42)\n    model1 = keras.Sequential()\n    model1.add(Dense(10, input_shape=(9,), activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(2, activation='softmax'))\n\n    model1.compile(loss = 'binary_crossentropy',  optimizer=opt ,metrics=[tf.keras.metrics.AUC(),\n                                                                          tf.keras.metrics.Accuracy(),\n                                                                          tf.keras.metrics.BinaryAccuracy(),\n                                                                          tf.keras.metrics.Recall(),\n                                                                          tf.keras.metrics.Precision(),])\n\n    history = model1.fit(X_scaled_train, y_onehot_train, epochs=350,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\n    plt.figure()\n    plt.title('Optimizer: {}'.format(opt))\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    keys = list(history.history.keys())\n    print('Loss: ',           min(history.history[keys[6]]))\n    print('AUC: ',            min(history.history[keys[7]]))\n    print('Binary Accuracy:', min(history.history[keys[9]]))\n    print('Recall: ',         min(history.history[keys[10]]))\n    print('Precision: ',      min(history.history[keys[11]]))\n    print('\\n')\n    tf.keras.backend.clear_session()\n    del model1","metadata":{"execution":{"iopub.status.busy":"2022-01-25T22:58:04.959044Z","iopub.execute_input":"2022-01-25T22:58:04.959408Z","iopub.status.idle":"2022-01-25T23:02:22.235299Z","shell.execute_reply.started":"2022-01-25T22:58:04.959377Z","shell.execute_reply":"2022-01-25T23:02:22.234005Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"* RMSProp seems a good option. lets tune the learning rate:","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\nlearning_rates = [0.01,0.001,0.0001,0.00001]\n\nact = 'sigmoid'\n\nfor lr in learning_rates:\n    tf.random.set_seed(42)\n    model1 = keras.Sequential()\n    model1.add(Dense(10, input_shape=(9,), activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(100, activation= act))\n    model1.add(Dense(2, activation='softmax'))\n    \n    \n    opt = RMSprop(learning_rate= lr, momentum=0.9) \n    \n    model1.compile(loss = 'binary_crossentropy',  optimizer=opt ,metrics=[tf.keras.metrics.AUC(),\n                                                                          tf.keras.metrics.Accuracy(),\n                                                                          tf.keras.metrics.BinaryAccuracy(),\n                                                                          tf.keras.metrics.Recall(),\n                                                                          tf.keras.metrics.Precision(),])\n\n    history = model1.fit(X_scaled_train, y_onehot_train, epochs=350,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\n    plt.figure()\n    plt.title('learning rate: {}'.format(lr))\n    plt.plot(history.history['loss'], label = 'Train')\n    plt.plot(history.history['val_loss'], label = 'Validation')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.show()\n    keys = list(history.history.keys())\n    print('Loss: ',           min(history.history[keys[6]]))\n    print('AUC: ',            min(history.history[keys[7]]))\n    print('Binary Accuracy:', min(history.history[keys[9]]))\n    print('Recall: ',         min(history.history[keys[10]]))\n    print('Precision: ',      min(history.history[keys[11]]))\n    print('\\n')\n    tf.keras.backend.clear_session()\n    del model1","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:02:22.237128Z","iopub.execute_input":"2022-01-25T23:02:22.237391Z","iopub.status.idle":"2022-01-25T23:04:47.726758Z","shell.execute_reply.started":"2022-01-25T23:02:22.237364Z","shell.execute_reply":"2022-01-25T23:04:47.724822Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"* We will continue with learning rate of 0.001. We should note that the convergence happens before iteration 50 with this learning rate.\n* NSo the best model has sigmoid as activation function, an architecture of 10/100/100/2, with binary cross entropy loss function, RMSProp and learning of 0.001 as optimizer.","metadata":{"execution":{"iopub.status.busy":"2022-01-25T21:42:10.339808Z","iopub.execute_input":"2022-01-25T21:42:10.340492Z","iopub.status.idle":"2022-01-25T21:42:10.346470Z","shell.execute_reply.started":"2022-01-25T21:42:10.340453Z","shell.execute_reply":"2022-01-25T21:42:10.345272Z"}}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\nlr = 0.001\nact = 'sigmoid'\ntf.random.set_seed(42)\n\nmodel1 = keras.Sequential()\nmodel1.add(Dense(10, input_shape=(9,), activation= act))\nmodel1.add(Dense(100, activation= act))\nmodel1.add(Dense(100, activation= act))\nmodel1.add(Dense(2, activation= 'softmax'))\n\n\nopt = RMSprop(learning_rate= lr, momentum=0.9) \n\nmodel1.compile(loss = 'binary_crossentropy',  optimizer=opt ,metrics=[tf.keras.metrics.AUC(),\n                                                                      tf.keras.metrics.Accuracy(),\n                                                                      tf.keras.metrics.BinaryAccuracy(),\n                                                                      tf.keras.metrics.Recall(),\n                                                                      tf.keras.metrics.Precision(),])\n\nhistory = model1.fit(X_scaled_train, y_onehot_train, epochs=50,validation_data=(X_scaled_val, y_onehot_val), verbose = 0)\n\nplt.figure()\nplt.title('learning rate: {}'.format(lr))\nplt.plot(history.history['loss'], label = 'Train')\nplt.plot(history.history['val_loss'], label = 'Validation')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()\nkeys = list(history.history.keys())\nprint('Loss: ',           min(history.history[keys[6]]))\nprint('AUC: ',            min(history.history[keys[7]]))\nprint('Binary Accuracy:', min(history.history[keys[9]]))\nprint('Recall: ',         min(history.history[keys[10]]))\nprint('Precision: ',      min(history.history[keys[11]]))\nprint('\\n')\ntf.keras.backend.clear_session()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:16:09.968197Z","iopub.execute_input":"2022-01-25T23:16:09.968715Z","iopub.status.idle":"2022-01-25T23:16:16.749629Z","shell.execute_reply.started":"2022-01-25T23:16:09.968683Z","shell.execute_reply":"2022-01-25T23:16:16.746662Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"* Performin the model on test dataset:","metadata":{}},{"cell_type":"code","source":"y_hat = model1.predict_classes(X_scaled_test)\ny_hat = y_hat.reshape(-1,1)\nresult = np.hstack((p_id.reshape(-1,1),y_hat))\ndf = pd.DataFrame(result, columns = ['PassengerId', 'Survived'])\ndf.to_csv('submission.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:17:40.083452Z","iopub.execute_input":"2022-01-25T23:17:40.083868Z","iopub.status.idle":"2022-01-25T23:17:40.155291Z","shell.execute_reply.started":"2022-01-25T23:17:40.083837Z","shell.execute_reply":"2022-01-25T23:17:40.153703Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### Final accuracy for ANN model: 0.78229\n\n* lets try nearest neighbor methods to see if there is any improvements\n","metadata":{}},{"cell_type":"code","source":"# Just saving dataset variations for future\n\n\nnp.savetxt(\"X_scaled_val.csv\", X_scaled_val, delimiter=\",\")\nnp.savetxt(\"X_scaled_train.csv\", X_scaled_train, delimiter=\",\")\nnp.savetxt(\"y_val.csv\", y_val, delimiter=\",\")\nnp.savetxt(\"y_train.csv\", y_train, delimiter=\",\")\nnp.savetxt(\"X_train.csv\", X_train, delimiter=\",\")\nnp.savetxt(\"y_onehot_val.csv\", y_onehot_val, delimiter=\",\")\nnp.savetxt(\"y_onehot_train.csv\", y_onehot_train, delimiter=\",\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:46:24.700312Z","iopub.execute_input":"2022-01-25T23:46:24.700921Z","iopub.status.idle":"2022-01-25T23:46:24.730468Z","shell.execute_reply.started":"2022-01-25T23:46:24.700881Z","shell.execute_reply":"2022-01-25T23:46:24.729423Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## Nearest Neighbor\n* first, adding some libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport datetime\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.decomposition import PCA","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:37:59.454579Z","iopub.execute_input":"2022-01-25T23:37:59.454963Z","iopub.status.idle":"2022-01-25T23:37:59.548253Z","shell.execute_reply.started":"2022-01-25T23:37:59.454931Z","shell.execute_reply":"2022-01-25T23:37:59.547271Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# for normalizing data\n\ndef X_norm(X):   \n        mean = np.average(X_train, axis=0)\n        sigma = np.var(X_train, axis=0)**0.5\n        X_new = (X - mean )/sigma\n        return X_new\n    \ndef y_norm(y):\n        mean = np.average(y_train, axis=0)\n        sigma = np.var(y_train, axis=0)**0.5\n        y_new = (y - mean )/sigma\n        return y_new\n    \nX_norm_train = X_norm(X_train)\nX_norm_val = X_norm(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:55:25.510300Z","iopub.execute_input":"2022-01-25T23:55:25.510670Z","iopub.status.idle":"2022-01-25T23:55:25.524475Z","shell.execute_reply.started":"2022-01-25T23:55:25.510639Z","shell.execute_reply":"2022-01-25T23:55:25.523227Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nmax_value = 0\nmax_k = 0\n\nfor k in range(2,10):\n    neigh = KNeighborsRegressor(n_neighbors=k, weights='distance', algorithm='kd_tree')\n    neigh.fit(X_norm_train, y_train)\n    print(\"R2 for k=\"+str(k)+\": \"+str(neigh.score(X_norm_val, y_val)))\n    if neigh.score(X_norm_val, y_val) > max_value:\n        max_value = neigh.score(X_norm_val, y_val)\n        max_k = k\n        \nprint('\\n')\nprint('maximum R2:',max_value)\nprint('best K:',max_k)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T23:55:45.822136Z","iopub.execute_input":"2022-01-25T23:55:45.822601Z","iopub.status.idle":"2022-01-25T23:55:45.953294Z","shell.execute_reply.started":"2022-01-25T23:55:45.822553Z","shell.execute_reply":"2022-01-25T23:55:45.950233Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"* So we would use K = 8\n* lets see if we can find optimal features for our work","metadata":{}},{"cell_type":"code","source":"from itertools import chain, combinations\n\ndef powerset(iterable):\n    s = list(iterable)\n    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n\nfeatures_list = list(X_norm_train.columns)\n\n# all possible variation of feature selection\nvariations = powerset(features_list)\nvariations = variations[1:]\n\nbest_performance = 0\nbest_features = ()\n\n\nfor var in variations:\n    neigh = KNeighborsRegressor(n_neighbors=8, weights='distance', algorithm='kd_tree')\n    neigh.fit(X_norm_train[list(var)], y_train)\n    score = neigh.score(X_norm_val[list(var)], y_val)\n    \n    if score > best_performance:\n        best_performance = score\n        best_features = var\n        \nprint('Best performance:',best_performance)\nprint('Best features:',best_features)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T00:05:10.159863Z","iopub.execute_input":"2022-01-26T00:05:10.160204Z","iopub.status.idle":"2022-01-26T00:05:13.621270Z","shell.execute_reply.started":"2022-01-26T00:05:10.160176Z","shell.execute_reply":"2022-01-26T00:05:13.620338Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"* That sounds reasonable to have age, sex, fare, and pclass to decide if the passenger has drowned or not, but the score is still too low!.\n* We wouldn't bother ourself to check the model on test dataset.\n* lets try a fixed radius to see if its a good idea or not.","metadata":{}},{"cell_type":"code","source":"neigh_r = RadiusNeighborsRegressor(radius=100)\nneigh_r.fit(X_norm_train[['Pclass', 'Age', 'Fare', 'Sex_female']], y_train)\nneigh_r.score(X_norm_val[['Pclass', 'Age', 'Fare', 'Sex_female']], y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T00:11:23.851759Z","iopub.execute_input":"2022-01-26T00:11:23.852241Z","iopub.status.idle":"2022-01-26T00:11:23.875123Z","shell.execute_reply.started":"2022-01-26T00:11:23.852214Z","shell.execute_reply":"2022-01-26T00:11:23.873930Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"So, we achieved the best result with ANN.","metadata":{}}]}